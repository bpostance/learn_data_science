{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b9c1dde0b2464ff6778d75d84efd3ba2ef21848"
   },
   "source": [
    "# Rule Based Relation-Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "85600930b325b2bc99c604b7535cf298e475c247"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# https://stackoverflow.com/a/47784683\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(eq=True)\n",
    "class Relation:\n",
    "    subj: str\n",
    "    rel: str\n",
    "    obj: str\n",
    "    type: str\n",
    "Relation('ben','is','here','DIRECT')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts =  [\n",
    "    # https://www.bbc.co.uk/news/world-us-canada-60177979\n",
    "    (\n",
    "        \"The US East Coast is hunkering down as a major blizzard hits the region for the first time in four years.\"\n",
    "        \"The storm is forecast to stretch from the Carolinas to Maine, packing hurricane-force winds in coastal parts.\"\n",
    "        \"Five states have declared emergencies.\"\n",
    "        \"Mayor Michelle Wu of Boston, a city that is no stranger to snowfall, said the storm could be 'historic'.\"\n",
    "        \"More than two feet of snow could fall in New England.\"\n",
    "        \"Weather officials also warn of flooding near the coast.\"\n",
    "        \"Over 5,000 US flights were cancelled between Friday and Sunday, according to FlightAware.\"\n",
    "        \"Forecasters say there is a chance the storm, known as a Nor\\'easter, will blanket the Boston area with up to 2ft (61cm) of snow.\"\n",
    "        ),\n",
    "    # https://www.bbc.co.uk/news/business-60163814\n",
    "    (  \n",
    "        \"Apple sales soared in the key Christmas shopping season, despite constraints due to a global shortage of microchips.\"\n",
    "        \"Sales at the iPhone giant rose 11% to a record $123.9bn (£92.6bn) in the October to December period, beating forecasts.\"\n",
    "        \"Shares jumped more than 4% in after-hours trade, as the report suggested the firm's pandemic boom is continuing.\"\n",
    "        \"Apple has seen purchases skyrocket during the pandemic as people spend more time online.\"\n",
    "        \"The firm's market value briefly hit the $3tn milestone in early January though its share price has slipped more recently amid weeks of market turmoil.\"\n",
    "        ),\n",
    "    # https://news.sky.com/story/staycation-frenzy-spurs-center-parcs-owner-to-prepare-4bn-sale-12527982\n",
    "    (\n",
    "        \"Sky News has learnt that Brookfield Property Partners, the Canadian property giant, is paving the way to sell Center Parcs UK potentially as soon as this year.\"\n",
    "        \"City sources said this weekend that Brookfield had engaged the accountancy firm PriceWaterhouseCoopers to assist with preparations for a sale process.\"\n",
    "        \"Investment banks have yet to be formally appointed to handle an auction, and one person close to the process said it was possible that Brookfield would decide to retain the business for a longer period if it did not secure a sufficiently attractive offer.\"\n",
    "        \"Center Parcs is one of the most famous brands in the British leisure industry, drawing millions of visitors annually to its five UK sites and the latest addition to its portfolio, at Longford Forest in Ireland.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2099872afd539fa28784a066f2ce0c06c42a2585"
   },
   "source": [
    "##  Part-Of-Speech (POS) tags\n",
    "\n",
    "In linguistics and grammar, A part of speech or part-of-speech (POS) is the category of a word that have similar grammatical properties. \n",
    "For instance \"nouns\" are words for real things like people, places and objects. Words that describe nouns are called \"adjectives\" such as: tall, smart, large. \n",
    "\n",
    "Applications in Natural Language Processing (NLP) apply linguistic rules and machine learning models to predict and assign which POS tags apply by evaluating word position and context. Popular NLP packages such as NLTK and spaCy include this functionality OOTB. To read more about POS, see this [POS summary](https://towardsdatascience.com/part-of-speech-tagging-for-beginners-3a0754b2ebba), the spaCy [documentation](https://spacy.io/usage/linguistic-features#pos-tagging) and [SO explanation](https://stackoverflow.com/questions/40288323/what-do-spacys-part-of-speech-and-dependency-tags-mean), and this [POS tag reference list](https://sites.google.com/site/partofspeechhelp/#TOC-Welcome).\n",
    "\n",
    "We can build a basic relation-extraction process by using grammar patterns / part of speech patterns to identify related nouns within a text. A simple rule might be:\n",
    "\n",
    "```\n",
    "Proper Noun - Verb - Proper Noun\n",
    "```\n",
    "\n",
    "Using spaCy, we can now iterate over each sentence and identify where this POS pattern occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd80efd1e651449ee42847f5859feadea27a15ee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nouns = ['NNP','NN','NNS']\n",
    "verbs = [\"VBZ\",\"VB\",\"VBG\"]\n",
    "relations = list()\n",
    "\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "    for e,sent in enumerate(doc.sents):\n",
    "        chain = list()\n",
    "        for a in sent:\n",
    "            if a.tag_ in nouns: # find first NOUND\n",
    "                chain.append(a)\n",
    "                for b in sent[a.i:]: # find ROOT, alternatively VERBS\n",
    "                    if (b.dep_ == 'ROOT') and len(chain) == 1:\n",
    "                        chain.append(b)\n",
    "                        for c in sent[b.i:]: # find second NOUN\n",
    "                            if c.tag_ in nouns and len(chain) == 2: \n",
    "                                chain.append(c)\n",
    "                                \n",
    "                                # reset chain and print result\n",
    "                                relations.append(chain)\n",
    "                                pos_chain = ' '.join([f\"{i} ({i.tag_}|{i.dep_})\" for i in sent[a.i:c.i+1]])\n",
    "                                print(chain,'\\n',pos_chain,'\\n')\n",
    "                                chain = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this simple approach seems OK at finding sentences that contain related entities.\n",
    "\n",
    "- US --- **hunkering** ---> Blizzard\n",
    "- Sales --- **soared** ---> Christmas\n",
    "- Sky --- **learnt** ---> Brookfield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Extraction\n",
    "\n",
    "The problem with the above approach is that it relies on an extensive list of *Part-Of-Speech* tag patterns. This won't scale for most problems as nouns and verbs come in a wide variety of forms and with modifiers etc. For instance, you generally want to capture compound term phrases and patterns such as:\n",
    "```\n",
    "Metro-North worker = NNP-HYPH-NNP\n",
    "Killed by = VBZ-IN\n",
    "```\n",
    "\n",
    "To improve our method we can:\n",
    " 1. Better capture and relfect things and objects - collectively named \"entities\".\n",
    "     \n",
    " \n",
    " 1. Develop POS patterns and rules to identify and extract relations between two or more entities. \n",
    "\n",
    " 1. Train a probabilistic model to identify relation triplets such as [Stanford, OLLIE - see reddit]\n",
    " \n",
    "### Capture entities\n",
    "\n",
    "A spacy pipeline with components for Named Entity Recognition (NER) and Noun Chunks is used to capture entities. Here, we could define some sensible rules or limit the number and type of entities to control what information will be represented in our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))\n",
    "    nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"))\n",
    "    print(nlp.pipe_names)\n",
    "except:\n",
    "    print(nlp.pipe_names)\n",
    "\n",
    "\n",
    "doc = nlp(texts[2])\n",
    "print('\\n', ' '.join([f\"{d} ({d.tag_}|{d.dep_})\" for d in doc]),'\\n')\n",
    "spacy.displacy.render(doc, style='ent')\n",
    "\n",
    "print('Entities:')\n",
    "for t in doc:\n",
    "    if t.ent_type_ != '': print('\\t',t,t.ent_type_)\n",
    "\n",
    "print('Noun chunks:')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print('\\t',chunk.text, )\n",
    "    #chunk.root.text, chunk.root.dep_,chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture relations\n",
    "\n",
    "For each sentence, relations can be extracted by iterating through the entity pairs and noun chunks, and yielding the ROOT VERB and other VERB terms. Spacy's dependency parser operates on each sentence in isolation and so it is not possible to extract relations across sentences with this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(span,ent_types=None,noun_chunks=False):\n",
    "    entities = dict()\n",
    "    if ent_types is None:\n",
    "        ent_types = ['ORG','PERSON','GPE','NORP','LOC','PRODUCT']\n",
    "        \n",
    "    if noun_chunks:\n",
    "        entities.update({int(f\"{i.start}{i.end}\"): {\"span\":i, \"type\":\"NOUN_CHUNK\"} for i in span.noun_chunks})\n",
    "        \n",
    "    entities.update({int(f\"{i.start}{i.end}\"): {\"span\":i, \"type\":i.label_} for i in span.ents if i.label_ in ent_types})\n",
    "    keys = sorted(entities)\n",
    "    return entities, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents: \n",
    "    s_term_pos = \" \".join([f\"{t} ({t.tag_}|{t.dep_})\" for t in sent])\n",
    "    entities, keys = get_entities(sent)\n",
    "\n",
    "    if len(keys) > 1:\n",
    "        pairs = [(x,y) for x,y in zip(keys,keys[1:])]\n",
    "        print('\\n',s_term_pos)\n",
    "        # print(keys,pairs)\n",
    "        for p in pairs:\n",
    "            start,end = entities[p[0]], entities[p[1]]\n",
    "            for w in doc[start['span'].start:end['span'].end]:\n",
    "                if w.tag_ in ['VBZ','VBN','VBG','VBD','VB']:\n",
    "                    print(f\"\\t>>>\\t\",f\"{start['span']} - {w}({w.tag_}|{w.dep_}) - {end['span']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.displacy.render(doc, style='ent')\n",
    "# spacy.displacy.render(sent, style='dep',)\n",
    "# spacy.explain('attr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better - but not great. There's a few additional rules we could add to expand and improve relationship capture. \n",
    "\n",
    "- capture direct ascendants and decedents https://spacy.io/usage/linguistic-features#navigating-around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direct_relations(sent,entities):\n",
    "    ent_text = [v['span'].text for v in entities.values()]\n",
    "    relations = list()\n",
    "\n",
    "    for v in entities.values():\n",
    "        \n",
    "        if v['span'].n_lefts > 0:\n",
    "            lf = [i for i in v['span'].lefts if i.text in ent_text]\n",
    "            if len(lf) > 0: \n",
    "                relations.append((lf[0],'=IS',v,'DIRECT'))\n",
    "        if v['span'].n_rights > 0:\n",
    "            rt = [i for i in v['span'].rights if i.text in ent_text]\n",
    "            if len(rt) > 0: \n",
    "                relations.append((v['span'].text,'=IS',rt[0],'DIRECT'))\n",
    "\n",
    "    if len(relations) > 0:\n",
    "        return relations\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "for sent in list(doc.sents)[:]: \n",
    "    s_term_pos = \" \".join([f\"{t} ({t.tag_}|{t.dep_})\" for t in sent])\n",
    "    entities, keys = get_entities(sent, noun_chunks=True)\n",
    "    \n",
    "    sentence_relations = get_direct_relations(sent,entities)\n",
    "    if sentence_relations is not None:\n",
    "        relations+=sentence_relations\n",
    "        \n",
    "        print(s_term_pos)\n",
    "        for r in sentence_relations:\n",
    "            print('\\t',r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get relationships between the subject and object term dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ent_pos_relations(doc, sentence_entities):\n",
    "    \"\"\"\n",
    "    Entity to POS relations\n",
    "    \"\"\"\n",
    "    SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "    OBJECTS = [\"pobj\",\"dobj\", \"dative\", \"attr\", \"oprd\"]\n",
    "    SUB_OBJ = SUBJECTS+OBJECTS\n",
    "    \n",
    "    feature = [  \"ROOT\",\n",
    "                 \"aux\",\"relcl\",\n",
    "                 \"acomp\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"nn\", \"nmod\", \"ccomp\", \"complm\",\n",
    "                 \"hmod\", \"infmod\", \"xcomp\", \"rcmod\", \"poss\",\" possessive\",\n",
    "                 \"compound\",\n",
    "                 \"prep\"]\n",
    "\n",
    "    relations = list()\n",
    "    for e in sentence_entities.values():\n",
    "        prev = list()\n",
    "        for t in doc[e['span'].end:sent.end]:\n",
    "            if t.dep_ in ['pobj','nsubj','dojb']: \n",
    "                terms = doc[e['span'].end:t.i]\n",
    "                rels = [t for t in terms if t.dep_ in feature if t.idx not in prev]\n",
    "                prev.extend([x.idx for x in rels]) # previous terms\n",
    "                \n",
    "                if len(rels) > 0:\n",
    "                    rels = ' '.join([x.text for x in rels])\n",
    "                    relations.append((e['span'].text,rels,t.text,'ENT_POS'))\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in list(doc.sents): \n",
    "    s_term_pos = \" \".join([f\"{t} ({t.tag_}|{t.dep_})\" for t in sent])\n",
    "    entities, keys = get_entities(sent, noun_chunks=True)\n",
    "    \n",
    "    sentence_relations = get_ent_pos_relations(doc,entities)\n",
    "    if sentence_relations is not None:\n",
    "        relations+=sentence_relations\n",
    "\n",
    "        print(s_term_pos)\n",
    "        for r in sentence_relations:\n",
    "            print('\\t',r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We could also consider [sentence simplification](https://stackoverflow.com/questions/65227103/clause-extraction-long-sentence-segmentation-in-python)\n",
    "\n",
    "## Notes\n",
    "spaCy uses the terms **head** and **child** to describe the words connected by a single arc in the dependency tree. The term **dep** is used for the arc label, which describes the type of syntactic relation that connects the child to the head.\n",
    " \n",
    "```\n",
    " for token in doc:\n",
    "    print(token,[token.head],list(token.children),end='\\r')\n",
    "```\n",
    "\n",
    "## References\n",
    " - [information-extraction-with-dominating-rules](https://github.com/philipperemy/information-extraction-with-dominating-rules)\n",
    " - [Pruning Knowledge Graphs](http://philipperemy.github.io/information-extract/)\n",
    " - [OLLIE](https://www.reddit.com/r/LanguageTechnology/comments/bovsf5/we_release_opiec_the_largest_open_information/)\n",
    " - [Clausie](https://github.com/mmxgn/clausiepy)\n",
    " - [Minie](https://github.com/mmxgn/miniepy/graphs/contributors)\n",
    " - [Knowledge Graph – A Powerful Data Science Technique to Mine Information from Text](https://www.analyticsvidhya.com/blog/2019/10/how-to-build-knowledge-graph-text-using-spacy/)\n",
    " - [Spacy subject-object extraction](https://github.com/NSchrading/intro-spacy-nlp/blob/master/subject_object_extraction.py)\n",
    "     -  [SO dependency parsing](https://stackoverflow.com/questions/39763091/how-to-extract-subjects-in-a-sentence-and-their-respective-dependent-phrases)\n",
    " - [sentence simplification](https://stackoverflow.com/questions/65227103/clause-extraction-long-sentence-segmentation-in-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
